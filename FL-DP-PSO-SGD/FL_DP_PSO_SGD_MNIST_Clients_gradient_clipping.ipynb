{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Julialunna/Artificial-Intelligence/blob/main/FL-DP-PSO-SGD/FL_DP_PSO_SGD_MNIST_Clients_gradient_clipping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pega o erro de cada um e compara. A partícula com menor erro pede o gbest."
      ],
      "metadata": {
        "id": "_CPCXVr31FDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install opacus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-88k_b5RxwsM",
        "outputId": "974f1a21-7f39-441d-f6f1-1c50fbbbf7d8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opacus\n",
            "  Downloading opacus-1.5.3-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.15 in /usr/local/lib/python3.11/dist-packages (from opacus) (1.26.4)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.11/dist-packages (from opacus) (2.6.0+cu124)\n",
            "Requirement already satisfied: scipy>=1.2 in /usr/local/lib/python3.11/dist-packages (from opacus) (1.14.1)\n",
            "Requirement already satisfied: opt-einsum>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from opacus) (3.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0->opacus)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0->opacus)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0->opacus)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0->opacus)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0->opacus)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0->opacus)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0->opacus)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0->opacus)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0->opacus)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0->opacus)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0->opacus) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0->opacus) (3.0.2)\n",
            "Downloading opacus-1.5.3-py3-none-any.whl (251 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.7/251.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, opacus\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 opacus-1.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split, Subset, TensorDataset\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "import random\n",
        "import csv\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import math\n",
        "from opacus import PrivacyEngine\n",
        "from opacus.utils.batch_memory_manager import BatchMemoryManager\n",
        "from opacus.accountants.utils import get_noise_multiplier\n",
        "from copy import deepcopy"
      ],
      "metadata": {
        "id": "AasvJNQd54yt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, device, input_size=28*28, hidden_size=256, num_classes=10):\n",
        "        super(MLP, self).__init__()\n",
        "        self.device = device\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, 128)\n",
        "        self.fc4 = nn.Linear(128, num_classes)\n",
        "        self.relu = nn.ReLU()  # ReLU é reutilizado\n",
        "        self.to(device)  # Move o modelo para o dispositivo especificado\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)  # Achata o tensor de entrada\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc4(x)  # Chamada correta da camada fc4\n",
        "        return x"
      ],
      "metadata": {
        "id": "DK3gmoasMO_-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definições dos hiperparâmetros\n",
        "NUM_CLIENTES = 5\n",
        "NUM_PARTICULAS = 25\n",
        "NUM_RODADAS = 10\n",
        "INERCIA, C1, C2 = 0.9, 0.8, 0.9\n",
        "EPSILON = 6/math.sqrt(50)\n",
        "DELTA = 1e-5\n",
        "SENSITIVITY = 4\n",
        "MAX_NORM = 2.0\n",
        "SUBSET_SIZE = 12000\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'training on {DEVICE}')\n",
        "\n",
        "# Criando o modelo global\n",
        "modelo_global = MLP(DEVICE, hidden_size=256)\n",
        "criterio = nn.CrossEntropyLoss()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOsKzlgWLN5r",
        "outputId": "bd64547e-3282-48d8-b9a5-384498b8ca9f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training on cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Seeds para reprodutibilidade\n",
        "random.seed(123)\n",
        "torch.manual_seed(123)\n",
        "torch.cuda.manual_seed(123)\n",
        "\n",
        "\n",
        "# Função para criar um subconjunto aleatório de um dataset\n",
        "def create_subset(dataset, num_clients):\n",
        "    indices = list(range(len(dataset)))  # Lista de todos os índices\n",
        "    random.shuffle(indices)  # Embaralha os índices para aleatoriedade\n",
        "\n",
        "    subset_size = len(indices) // num_clients  # Tamanho de cada subconjunto\n",
        "    subsets = [Subset(dataset, indices[i * subset_size : (i + 1) * subset_size]) for i in range(num_clients)]\n",
        "\n",
        "    dataloaders = [\n",
        "        DataLoader(subset, batch_size=300, shuffle=False)\n",
        "        for subset in subsets\n",
        "    ]\n",
        "    return dataloaders\n",
        "\n",
        "    return dataloaders\n",
        "def add_module_prefix(state_dict):\n",
        "    new_state_dict = {}\n",
        "    for key, value in state_dict.items():\n",
        "        new_key = f\"_module.{key}\"  # Adiciona o prefixo `_module`\n",
        "        new_state_dict[new_key] = value\n",
        "    return new_state_dict\n",
        "\n",
        "class Particula:\n",
        "    def __init__(self, particle_id, modelo_cliente):\n",
        "        self.particle_id = particle_id\n",
        "        self.pesos = {f\"_module.{key}\": value.clone() for key, value in modelo_cliente.state_dict().items()}\n",
        "        self.melhor_pesos = copy.deepcopy(self.pesos)  # pbest (melhor posição da partícula)\n",
        "        self.melhor_erro = float('inf')  # Melhor erro alcançado\n",
        "        self.velocidade = {name: torch.zeros_like(param) for name, param in self.pesos.items()}  # Velocidade do PSO\n",
        "        self.device = modelo_cliente.device  # Dispositivo do modelo\n",
        "\n",
        "    def atualizar_pso(self, global_best_pesos, INERCIA, C1, C2, rodada):\n",
        "        \"\"\"Atualiza os pesos da partícula usando a equação do PSO.\"\"\"\n",
        "        global_best_pesos_ajustados = global_best_pesos\n",
        "        if(rodada == 0):\n",
        "          global_best_pesos_ajustados = {f\"_module.{key}\": value for key, value in global_best_pesos.items()}\n",
        "        for name in self.pesos:\n",
        "            local_rand = random.random()\n",
        "            global_rand = random.random()\n",
        "            self.velocidade[name] = (\n",
        "                INERCIA * self.velocidade[name] +\n",
        "                C1 * local_rand * (self.melhor_pesos[name] - self.pesos[name]) +\n",
        "                C2 * global_rand * (global_best_pesos_ajustados[name] - self.pesos[name])\n",
        "            )\n",
        "            self.pesos[name] += self.velocidade[name]\n",
        "            ''' sigma = SENSITIVITY * torch.sqrt((2.0 * torch.log(torch.tensor(1.0 / DELTA))).clone().detach()) / EPSILON\n",
        "\n",
        "              # Gerar ruído diretamente com a distribuição normal do PyTorch (muito mais eficiente!)\n",
        "              noise = torch.normal(mean=0, std=sigma, size=self.velocidade[name].shape, device=self.device)\n",
        "              self.velocidade[name] += noise\n",
        "              #clipping velocity\n",
        "              self.velocidade[name] = torch.clamp(self.velocidade[name], -MAX_VELOCITY, MAX_VELOCITY)'''\n",
        "\n",
        "    def avaliar_perda(self, modelo_cliente, criterio, dados):\n",
        "        \"\"\"Calcula a perda da partícula no modelo do cliente.\"\"\"\n",
        "        #pesos_ajustados = {f\"_module.{key}\": value for key, value in self.pesos.items()}\n",
        "\n",
        "        modelo_cliente.load_state_dict(self.pesos)  # Aplica os pesos da partícula no modelo do cliente\n",
        "        modelo_cliente.eval()\n",
        "        total_loss = 0\n",
        "        device = next(modelo_cliente.parameters()).device\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in dados:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = modelo_cliente(inputs)\n",
        "                loss = criterio(outputs, labels)\n",
        "                total_loss += loss.item()\n",
        "\n",
        "        return total_loss / len(dados)\n",
        "\n",
        "\n",
        "class Cliente:\n",
        "    def __init__(self, cliente_id, modelo_global, dados, num_particulas=5):\n",
        "        self.cliente_id = cliente_id\n",
        "        self.modelo = copy.deepcopy(modelo_global)  # Cada cliente tem seu próprio modelo\n",
        "        self.dados = dados\n",
        "        self.num_particulas = num_particulas\n",
        "        self.particulas = []\n",
        "        self.melhor_particula = None\n",
        "        self.inicializar_particulas(num_particulas)\n",
        "        self.otimizador = optim.Adam(self.modelo.parameters(), lr=0.009, weight_decay=1e-5)\n",
        "\n",
        "    def inicializar_particulas(self, num_particulas):\n",
        "        \"\"\"Cria um conjunto de partículas associadas ao cliente.\"\"\"\n",
        "        self.particulas = [Particula(i, self.modelo) for i in range(num_particulas)]\n",
        "\n",
        "    def treinar_com_pso(self, INERCIA, C1, C2, global_best_pesos, criterio, rodada):\n",
        "        \"\"\"Treina as partículas usando PSO e atualiza a melhor partícula local.\"\"\"\n",
        "\n",
        "        for particula in self.particulas:\n",
        "            particula.atualizar_pso(global_best_pesos, INERCIA, C1, C2, rodada)\n",
        "            erro = particula.avaliar_perda(self.modelo, criterio, self.dados)\n",
        "            if erro < particula.melhor_erro:\n",
        "                particula.melhor_erro = erro\n",
        "                particula.melhor_pesos = copy.deepcopy(particula.pesos)\n",
        "\n",
        "        self.selecionar_melhor_particula()\n",
        "\n",
        "    def refinar_com_adam(self, criterio, EPSILON, DELTA, MAX_NORM, SENSITIVITY):\n",
        "        \"\"\"Refina os pesos da melhor partícula usando Adam.\"\"\"\n",
        "        self.modelo.load_state_dict(self.melhor_particula.melhor_pesos)\n",
        "        #train_loader = DataLoader(self.dados, batch_size=32, shuffle=True)\n",
        "        device = next(self.modelo.parameters()).device\n",
        "        self.modelo.train()\n",
        "        for i in range(2):  # 10 épocas de refinamento com Adam\n",
        "          with BatchMemoryManager(data_loader=self.dados, max_physical_batch_size=300, optimizer=self.otimizador) as new_data_loader:\n",
        "            for inputs, labels in new_data_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                self.otimizador.zero_grad()\n",
        "                outputs = self.modelo(inputs)\n",
        "                loss = criterio(outputs, labels)\n",
        "                loss.backward()\n",
        "                self.otimizador.step()\n",
        "                '''sigma = SENSITIVITY * torch.sqrt((2.0 * torch.log(torch.tensor(1.0 / DELTA))).clone().detach()) / EPSILON\n",
        "                if i==4:\n",
        "                  # Percorrer todos os parâmetros do modelo e adicionar ruído aos gradientes\n",
        "                  for param in self.modelo.parameters():\n",
        "                      if param.grad is not None:\n",
        "                          torch.nn.utils.clip_grad_norm_(self.modelo.parameters(), MAX_NORM)\n",
        "                          noise = torch.normal(mean=0, std=sigma, size=param.grad.shape, device=param.grad.device)\n",
        "                          param.grad += noise  # 🔹 Adiciona ruído diretamente ao gradiente'''\n",
        "        epsilon = privacy_engine.get_epsilon(delta = 1e-5)\n",
        "        print(f\"Orçamento de privacidade (epsilon) Cliente {self.cliente_id}: {epsilon}\")\n",
        "        # Atualiza os pesos da melhor partícula com os pesos refinados pelo Adam\n",
        "        self.melhor_particula.pesos = copy.deepcopy(self.modelo.state_dict())\n",
        "\n",
        "    def selecionar_melhor_particula(self):\n",
        "        \"\"\"Seleciona a melhor partícula do cliente.\"\"\"\n",
        "        self.melhor_particula = min(self.particulas, key=lambda p: p.melhor_erro)\n",
        "\n",
        "\n",
        "def treinar_federado(modelo_global, clientes, criterio, num_rodadas, INERCIA, C1, C2, testloader, EPSILON, DELTA, MAX_NORM, SENSITIVITY):\n",
        "    \"\"\"Treina os clientes localmente e sincroniza com o servidor central, validando a acurácia.\"\"\"\n",
        "\n",
        "    melhor_peso_global = copy.deepcopy(modelo_global.state_dict())  # Inicializa com o modelo global\n",
        "    melhor_erro_global = float('inf')\n",
        "    for rodada in range(num_rodadas):\n",
        "        resultados_rodada = []\n",
        "\n",
        "\n",
        "        for cliente in clientes:\n",
        "            cliente.treinar_com_pso(INERCIA, C1, C2, melhor_peso_global, criterio, rodada)  # Treino com PSO\n",
        "            cliente.refinar_com_adam(criterio, EPSILON, DELTA, MAX_NORM, SENSITIVITY)  # Refinamento com Adam\n",
        "            erro_cliente = cliente.melhor_particula.melhor_erro  # Obtém o melhor erro do cliente\n",
        "            resultados_rodada.append((cliente.cliente_id, erro_cliente))\n",
        "\n",
        "        resultados_sorted = sorted(resultados_rodada, key=lambda x: x[1])\n",
        "        top_3_results = resultados_sorted[:3]\n",
        "\n",
        "        melhor_cliente = random.choice(top_3_results)\n",
        "        melhor_cliente_id = melhor_cliente[0]\n",
        "        melhor_erro_cliente = melhor_cliente[1]\n",
        "\n",
        "        melhor_peso_global = copy.deepcopy(clientes[melhor_cliente_id].melhor_particula.pesos)\n",
        "        melhor_peso_global_ajustado = {key.replace(\"_module.\", \"\"): value for key, value in melhor_peso_global.items()}\n",
        "        melhor_erro_global = melhor_erro_cliente\n",
        "\n",
        "        modelo_global.load_state_dict(melhor_peso_global_ajustado)\n",
        "\n",
        "        test_loss, test_accuracy = avaliar_modelo(modelo_global, criterio, testloader)\n",
        "\n",
        "\n",
        "        print(f\"Rodada {rodada+1}/{num_rodadas}: Cliente {melhor_cliente_id} enviou os pesos.\")\n",
        "        print(f\"Erro Global Atualizado: {melhor_erro_global:.4f}\")\n",
        "        print(f\"Teste -> Perda: {test_loss:.4f}, Acurácia: {test_accuracy:.2f}%\\n\")\n",
        "\n",
        "    print(\"Treinamento Federado Finalizado!\")\n",
        "\n",
        "def avaliar_modelo(modelo, criterio, testloader):\n",
        "    \"\"\"Avalia o modelo global no conjunto de teste.\"\"\"\n",
        "    modelo.eval()  # Modo de avaliação\n",
        "\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in testloader:\n",
        "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "            outputs = modelo(inputs)\n",
        "            loss = criterio(outputs, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            correct += (outputs.argmax(1) == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "    test_loss = total_loss / len(testloader)\n",
        "    test_accuracy = (correct / total_samples) * 100\n",
        "\n",
        "    return test_loss, test_accuracy\n",
        "\n",
        "\n",
        "mnist_train = torchvision.datasets.MNIST(root='./data', train=True, download=True)\n",
        "X_train = mnist_train.data.view(-1, 28*28).numpy()  # Flatten (Transforma 28x28 em 784)\n",
        "y_train = mnist_train.targets.numpy()  # Labels\n",
        "\n",
        "mnist_test = torchvision.datasets.MNIST(root='./data', train=False, download=True)\n",
        "X_test = mnist_test.data.view(-1, 28*28).numpy()  # Flatten (Transforma 28x28 em 784)\n",
        "y_test = mnist_test.targets.numpy()  # Labels\n",
        "\n",
        "# Dividir treino e teste manualmente como no Iris\n",
        "\n",
        "# Normalizar como no Iris\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Converter para tensores\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Criar datasets como no Iris\n",
        "trainset = TensorDataset(X_train, y_train)\n",
        "testset = TensorDataset(X_test, y_test)\n",
        "\n",
        "\n",
        "trainloaders = create_subset(trainset, NUM_CLIENTES)\n",
        "\n",
        "testloader = DataLoader(testset, batch_size=300, shuffle=False)\n",
        "\n",
        "noise = get_noise_multiplier(target_epsilon = 3,\n",
        "                             target_delta = 1e-5,\n",
        "                             sample_rate = 300/12000,\n",
        "                             epochs = 10)\n",
        "\n",
        "# Criando os clientes\n",
        "clientes = [Cliente(i, modelo_global, trainloaders[i], NUM_PARTICULAS) for i in range(NUM_CLIENTES)]\n",
        "privacy_engine = PrivacyEngine()\n",
        "for cliente in clientes:\n",
        "\n",
        "    cliente.modelo, cliente.otimizador, cliente.dados = privacy_engine.make_private(\n",
        "        module=cliente.modelo,\n",
        "        optimizer=cliente.otimizador,\n",
        "        data_loader=cliente.dados,  # Seu DataLoader\n",
        "        noise_multiplier=noise,      # Controle o nível de ruído\n",
        "        max_grad_norm=1.0,         # Clipping dos gradientes\n",
        "    )\n",
        "\n",
        "# Executando o treinamento federado\n",
        "treinar_federado(modelo_global, clientes, criterio, NUM_RODADAS, INERCIA, C1, C2, testloader, EPSILON, DELTA, MAX_NORM, SENSITIVITY)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 688
        },
        "id": "nkL-eId7AF03",
        "outputId": "133e037b-13d4-47d4-8d7a-3b84be71db90"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 53.5MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.97MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 12.8MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 6.99MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/privacy_engine.py:152: UserWarning: PrivacyEngine detected new dataset object. Was: <torch.utils.data.dataset.Subset object at 0x7ef0e8904210>, got: <torch.utils.data.dataset.Subset object at 0x7ef0e88eddd0>. Privacy accounting works per dataset, please initialize new PrivacyEngine if you're using different dataset. You can ignore this warning if two datasets above represent the same logical dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/privacy_engine.py:152: UserWarning: PrivacyEngine detected new dataset object. Was: <torch.utils.data.dataset.Subset object at 0x7ef0e8904210>, got: <torch.utils.data.dataset.Subset object at 0x7ef0e802d550>. Privacy accounting works per dataset, please initialize new PrivacyEngine if you're using different dataset. You can ignore this warning if two datasets above represent the same logical dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/privacy_engine.py:152: UserWarning: PrivacyEngine detected new dataset object. Was: <torch.utils.data.dataset.Subset object at 0x7ef0e8904210>, got: <torch.utils.data.dataset.Subset object at 0x7ef0e802df10>. Privacy accounting works per dataset, please initialize new PrivacyEngine if you're using different dataset. You can ignore this warning if two datasets above represent the same logical dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/privacy_engine.py:152: UserWarning: PrivacyEngine detected new dataset object. Was: <torch.utils.data.dataset.Subset object at 0x7ef0e8904210>, got: <torch.utils.data.dataset.Subset object at 0x7ef0e802ca10>. Privacy accounting works per dataset, please initialize new PrivacyEngine if you're using different dataset. You can ignore this warning if two datasets above represent the same logical dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Orçamento de privacidade (epsilon) Cliente 0: 1.33968117025341\n",
            "Orçamento de privacidade (epsilon) Cliente 1: 1.7632286460470201\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-9ab18b4d7219>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;31m# Executando o treinamento federado\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m \u001b[0mtreinar_federado\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelo_global\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclientes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_RODADAS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mINERCIA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPSILON\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDELTA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_NORM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSENSITIVITY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-9ab18b4d7219>\u001b[0m in \u001b[0;36mtreinar_federado\u001b[0;34m(modelo_global, clientes, criterio, num_rodadas, INERCIA, C1, C2, testloader, EPSILON, DELTA, MAX_NORM, SENSITIVITY)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcliente\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclientes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mcliente\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtreinar_com_pso\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mINERCIA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmelhor_peso_global\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrodada\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Treino com PSO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             \u001b[0mcliente\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefinar_com_adam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcriterio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPSILON\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDELTA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_NORM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSENSITIVITY\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Refinamento com Adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m             \u001b[0merro_cliente\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcliente\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmelhor_particula\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmelhor_erro\u001b[0m  \u001b[0;31m# Obtém o melhor erro do cliente\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0mresultados_rodada\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcliente\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcliente_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merro_cliente\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-9ab18b4d7219>\u001b[0m in \u001b[0;36mrefinar_com_adam\u001b[0;34m(self, criterio, EPSILON, DELTA, MAX_NORM, SENSITIVITY)\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodelo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0motimizador\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 '''sigma = SENSITIVITY * torch.sqrt((2.0 * torch.log(torch.tensor(1.0 / DELTA))).clone().detach()) / EPSILON\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You are trying to call the hook of a dead Module!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/opacus/grad_sample/grad_sample_module.py\u001b[0m in \u001b[0;36mcapture_backprops_hook\u001b[0;34m(self, module, _forward_input, forward_output, loss_reduction, batch_first)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0mgrad_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_sampler_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackprops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrad_samples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m             create_or_accumulate_grad_sample(\n\u001b[0m\u001b[1;32m    341\u001b[0m                 \u001b[0mparam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_batch_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_batch_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/opacus/grad_sample/grad_sample_module.py\u001b[0m in \u001b[0;36mcreate_or_accumulate_grad_sample\u001b[0;34m(param, grad_sample, max_batch_len)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_grad_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgrad_sample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgrad_sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             param._current_grad_sample = torch.zeros(\n\u001b[0m\u001b[1;32m     59\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmax_batch_len\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgrad_sample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrad_sample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " '''sigma = SENSITIVITY * torch.sqrt((2.0 * torch.log(torch.tensor(1.0 / DELTA))).clone().detach()) / EPSILON\n",
        "              if i==4:\n",
        "                # Percorrer todos os parâmetros do modelo e adicionar ruído aos gradientes\n",
        "                for param in self.modelo.parameters():\n",
        "                    if param.grad is not None:\n",
        "                        torch.nn.utils.clip_grad_norm_(self.modelo.parameters(), MAX_NORM)\n",
        "                        noise = torch.normal(mean=0, std=sigma, size=param.grad.shape, device=param.grad.device)\n",
        "                        param.grad += noise  # 🔹 Adiciona ruído diretamente ao gradiente'''"
      ],
      "metadata": {
        "id": "w7tgCKOD85iX"
      }
    }
  ]
}