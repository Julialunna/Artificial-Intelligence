{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Julialunna/Artificial-Intelligence/blob/main/FL_DP_PSO_SGD/FL_DP_PSO_SGD_Iris_gradient_clipping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pega o erro de cada um e compara. A partícula com menor erro pede o gbest."
      ],
      "metadata": {
        "id": "_CPCXVr31FDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split, Subset, TensorDataset\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "import random\n",
        "import csv\n",
        "import torchvision.models as models\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "aFrNmR3h_7j6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Hiperparâmetros\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS_PER_CLIENT = 10\n",
        "NUM_CLIENTS = 10\n",
        "NUM_ROUNDS = 100\n",
        "INERTIA, C1, C2 = 0.9, 0.8, 0.9\n",
        "# LEARNING_RATE = 0.001\n",
        "EPSILON = 5\n",
        "DELTA = 1e-5\n",
        "SENSITIVITY = 4\n",
        "MAX_NORM = 2\n",
        "# Configuração do dispositivo\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'training on {DEVICE}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdm3qomH_9Gy",
        "outputId": "740aa7fd-5235-446a-fabc-a6ed76541cfc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training on cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Seeds para reprodutibilidade\n",
        "random.seed(123)\n",
        "torch.manual_seed(123)\n",
        "torch.cuda.manual_seed(123)\n",
        "\n",
        "\n",
        "# Função para criar um subconjunto aleatório de um dataset\n",
        "def create_subset(dataset, subset_size):\n",
        "    indices = list(range(len(dataset)))\n",
        "    subset_indices = random.sample(indices, subset_size)\n",
        "    return Subset(dataset, subset_indices)\n",
        "\n",
        "# # Função para salvar os resultados em um arquivo CSV\n",
        "# def write_csv(algorithm_name, architecture, results):\n",
        "#     file_name = f'{algorithm_name}_{architecture}_PreTreinado_CIFAR10_output.csv'\n",
        "#     with open(file_name, 'w', encoding='utf-8', newline='') as f:\n",
        "#         writer = csv.writer(f)\n",
        "#         writer.writerow([\"Epoch\", \"Test Loss\", \"Test Accuracy\"])\n",
        "#         for i, result in enumerate(results):\n",
        "#             writer.writerow([i + 1] + result)\n",
        "\n",
        "\n",
        "class MLP(nn.Module): # Class MLP heritage from Module\n",
        "  def __init__(self, input_dim, output_dim): # Constructor. 'self' in Python is similar to 'this' in C++.\n",
        "    super(MLP, self).__init__() # Call Module constructor first to guarantee initialization\n",
        "    # Defining each layer: type and number of neurons\n",
        "    self.fc1 = nn.Linear(input_dim, 16)\n",
        "    self.fc2 = nn.Linear(16, 8)\n",
        "    self.fc3 = nn.Linear(8, 8)\n",
        "    self.fc4 = nn.Linear(8, output_dim)\n",
        "\n",
        "    # Defining each activation function\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = F.relu(self.fc3(x))\n",
        "    x = self.fc4(x) # No activation function\n",
        "    return x\n",
        "\n",
        "class Particle:\n",
        "    def __init__(self, particle_id, model, data):\n",
        "        self.particle_id = particle_id\n",
        "        self.model = model.to(DEVICE)\n",
        "        self.data = data  # `self.data` agora armazena o dataset, não o DataLoader\n",
        "        self.local_best_model = copy.deepcopy(model.state_dict())\n",
        "        self.local_best_score = float('inf')\n",
        "        self.velocity = {name: torch.zeros_like(param) for name, param in model.named_parameters()}\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.007, weight_decay=1e-5)\n",
        "\n",
        "    def train_particle_adam(self, criterion, EPSILON, DELTA, SENSITIVITY, MAX_NORM):\n",
        "        self.model.train()\n",
        "        train_loader = DataLoader(self.data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "        for _ in range(EPOCHS_PER_CLIENT):\n",
        "            for inputs, labels in train_loader:\n",
        "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "                self.optimizer.zero_grad()\n",
        "                outputs = self.model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "\n",
        "                sigma = SENSITIVITY * torch.sqrt((2.0 * torch.log(torch.tensor(1.0 / DELTA))).clone().detach()) / EPSILON\n",
        "\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), MAX_NORM)\n",
        "                for param in self.model.parameters():\n",
        "                    if param.grad is not None:\n",
        "                        noise = torch.normal(mean=0, std=sigma, size=param.grad.shape, device=param.grad.device)\n",
        "                        param.grad += noise\n",
        "\n",
        "                self.optimizer.step()\n",
        "\n",
        "        val_loss = self.evaluate_loss(criterion)\n",
        "        if val_loss < self.local_best_score:\n",
        "            self.local_best_score = val_loss\n",
        "            self.local_best_model = copy.deepcopy(self.model.state_dict())\n",
        "        return self.particle_id, val_loss\n",
        "\n",
        "    def train_particle_pso(self, global_best_model, INERTIA, C1, C2):\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if param.grad is None:\n",
        "                continue\n",
        "            local_rand = random.random()\n",
        "            global_rand = random.random()\n",
        "            self.velocity[name] = (\n",
        "                INERTIA * self.velocity[name]\n",
        "                + C1 * local_rand * (self.local_best_model[name].to(DEVICE) - param.data)\n",
        "                + C2 * global_rand * (global_best_model[name].to(DEVICE) - param.data)\n",
        "            )\n",
        "            param.data += self.velocity[name]\n",
        "            # param.data = param.data - LEARNING_RATE * param.grad\n",
        "    def evaluate_loss(self, criterion):\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        val_loader = DataLoader(self.data, batch_size=BATCH_SIZE, shuffle=False)\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "                outputs = self.model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                total_loss += loss.item()\n",
        "        return total_loss / len(val_loader)\n",
        "\n",
        "\n",
        "def train_federated():\n",
        "    # transform = transforms.Compose([\n",
        "    #     transforms.Resize((224, 224)),\n",
        "    #     transforms.ToTensor(),\n",
        "    #     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    # ])\n",
        "\n",
        "    # trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "    # trainloaders = [create_subset(trainset, 5000) for _ in range(NUM_CLIENTS)]\n",
        "\n",
        "    # testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "    # testloader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    iris = datasets.load_iris() # Load dataset Iris. Classification. 150 instances, 4 features and 3 classes.\n",
        "    X = iris.data # X receives all features of each flower\n",
        "    y = iris.target # y receives all classes of each flower\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Spliting data: 20% to test and 80% to training\n",
        "\n",
        "    # Normalizing data\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    # Transforming data in tensor\n",
        "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "    y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "    y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "        # Criando datasets\n",
        "    trainset = TensorDataset(X_train, y_train)\n",
        "    testset = TensorDataset(X_test, y_test)\n",
        "\n",
        "    # Criando subconjuntos para cada cliente\n",
        "    SUBSET_SIZE = 12\n",
        "    trainloaders = [create_subset(trainset, SUBSET_SIZE) for _ in range(NUM_CLIENTS)]\n",
        "\n",
        "    # Criando DataLoader para o conjunto de teste\n",
        "    testloader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    input_dim = X_train.shape[1]  # Número de atributos (4 para o Iris)\n",
        "    output_dim = len(torch.unique(y_train))  # Número de classes (3 para o Iris)\n",
        "\n",
        "    server_model = MLP(input_dim, output_dim).to(DEVICE)\n",
        "    particles = [Particle(i, copy.deepcopy(server_model), trainloaders[i]) for i in range(NUM_CLIENTS)]\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    global_best_score = float('inf')\n",
        "    global_best_model = copy.deepcopy(server_model.state_dict())\n",
        "    server_evaluate_acc = []\n",
        "\n",
        "    for epoch in range(NUM_ROUNDS):\n",
        "        server_result = []\n",
        "        for particle in particles:\n",
        "            if epoch > 0:\n",
        "                particle.train_particle_pso(global_best_model, INERTIA, C1, C2)\n",
        "            pid, val_loss = particle.train_particle_adam(criterion, EPSILON, DELTA, SENSITIVITY, MAX_NORM)\n",
        "            server_result.append((pid, val_loss))\n",
        "\n",
        "        # print(server_result)\n",
        "        sorted_result = sorted(server_result, key=lambda x: x[1])\n",
        "        # print(sorted_result)\n",
        "        top_3_particles = sorted_result[:3]\n",
        "        # print(top_3_particles)\n",
        "\n",
        "        # best_particle = min(server_result, key=lambda x: x[1])\n",
        "        best_particle = random.choice(top_3_particles)\n",
        "        best_particle_id = best_particle[0]  # ID da melhor partícula\n",
        "        global_best_score = best_particle[1]\n",
        "        global_best_model = copy.deepcopy(particles[best_particle_id].local_best_model)\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "          print(f'Epoch {epoch + 1}/{NUM_ROUNDS}, Best Particle Loss: {global_best_score:.4f}')\n",
        "          print(f\"Round {epoch + 1}: Partícula {best_particle_id} está enviando os pesos.\")\n",
        "\n",
        "        server_model.load_state_dict(global_best_model)\n",
        "        server_model.eval()\n",
        "        total_loss, correct = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in testloader:\n",
        "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "                outputs = server_model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                total_loss += loss.item()\n",
        "                correct += (outputs.argmax(1) == labels).type(torch.float).sum().item()\n",
        "\n",
        "        test_loss = total_loss / len(testloader)\n",
        "        test_accuracy = (correct / len(testset))*100\n",
        "        server_evaluate_acc.append([test_loss, test_accuracy])\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "          print(f'Epoch {epoch + 1}/{NUM_ROUNDS}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
        "          print('---')\n",
        "\n",
        "    #write_csv(\"FLPSO-SGD3\", architecture, server_evaluate_acc)\n",
        "\n",
        "# Exemplo de execução\n",
        "train_federated()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkL-eId7AF03",
        "outputId": "3c11e889-ed44-49f3-f76e-e5cf1eecf90e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/100, Best Particle Loss: 1.1132\n",
            "Round 10: Partícula 3 está enviando os pesos.\n",
            "Epoch 10/100, Test Loss: 1.1615, Test Accuracy: 30.00%\n",
            "---\n",
            "Epoch 20/100, Best Particle Loss: 0.9220\n",
            "Round 20: Partícula 0 está enviando os pesos.\n",
            "Epoch 20/100, Test Loss: 1.0036, Test Accuracy: 33.33%\n",
            "---\n",
            "Epoch 30/100, Best Particle Loss: 0.3337\n",
            "Round 30: Partícula 8 está enviando os pesos.\n",
            "Epoch 30/100, Test Loss: 0.5968, Test Accuracy: 70.00%\n",
            "---\n",
            "Epoch 40/100, Best Particle Loss: 0.2709\n",
            "Round 40: Partícula 8 está enviando os pesos.\n",
            "Epoch 40/100, Test Loss: 0.4596, Test Accuracy: 73.33%\n",
            "---\n",
            "Epoch 50/100, Best Particle Loss: 0.1925\n",
            "Round 50: Partícula 8 está enviando os pesos.\n",
            "Epoch 50/100, Test Loss: 0.5223, Test Accuracy: 70.00%\n",
            "---\n",
            "Epoch 60/100, Best Particle Loss: 0.1043\n",
            "Round 60: Partícula 5 está enviando os pesos.\n",
            "Epoch 60/100, Test Loss: 0.1507, Test Accuracy: 96.67%\n",
            "---\n",
            "Epoch 70/100, Best Particle Loss: 0.0667\n",
            "Round 70: Partícula 2 está enviando os pesos.\n",
            "Epoch 70/100, Test Loss: 0.5810, Test Accuracy: 93.33%\n",
            "---\n",
            "Epoch 80/100, Best Particle Loss: 0.4750\n",
            "Round 80: Partícula 5 está enviando os pesos.\n",
            "Epoch 80/100, Test Loss: 0.0475, Test Accuracy: 96.67%\n",
            "---\n",
            "Epoch 90/100, Best Particle Loss: 0.3112\n",
            "Round 90: Partícula 6 está enviando os pesos.\n",
            "Epoch 90/100, Test Loss: 0.1721, Test Accuracy: 90.00%\n",
            "---\n",
            "Epoch 100/100, Best Particle Loss: 0.9252\n",
            "Round 100: Partícula 2 está enviando os pesos.\n",
            "Epoch 100/100, Test Loss: 0.5961, Test Accuracy: 93.33%\n",
            "---\n"
          ]
        }
      ]
    }
  ]
}