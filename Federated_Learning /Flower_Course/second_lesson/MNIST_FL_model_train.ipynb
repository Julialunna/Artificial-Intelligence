{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyME3qXiehRH8mfnNb2yos8z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Julialunna/Artificial-Intelligence/blob/main/Federated_Learning%20/Flower_Course/second_lesson/MNIST_FL_model_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        },
        "collapsed": true,
        "id": "qzsUGu8ammih",
        "outputId": "ba99a103-c743-47a2-a9d6-039ba77352ff"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6fe5e1cd-8e56-418d-851d-4175989d9b97\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6fe5e1cd-8e56-418d-851d-4175989d9b97\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving utils2.py to utils2.py\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'utils2.py': b'\"\"\"\\nUtility functions and classes for Jupyter Notebooks lessons. \\n\"\"\"\\n\\nfrom collections import OrderedDict\\nfrom typing import List, Tuple, Dict, Optional\\nfrom flwr.common import Metrics, NDArrays, Scalar\\nimport torch\\nimport torch.nn as nn\\nfrom torch.utils.data import Subset, DataLoader, random_split\\nimport torch.optim as optim\\nfrom torchvision import datasets, transforms\\nimport matplotlib.pyplot as plt\\nfrom sklearn.metrics import confusion_matrix\\nimport seaborn as sns\\nimport numpy as np\\nimport logging\\nfrom flwr.common.logger import console_handler, log\\nfrom logging import INFO, ERROR\\n\\n\\nclass InfoFilter(logging.Filter):\\n    def filter(self, record):\\n        return record.levelno == INFO\\n\\n\\nconsole_handler.setLevel(INFO)\\nconsole_handler.addFilter(InfoFilter())\\n\\ntransform = transforms.Compose(\\n    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\\n)\\n\\n# To filter logging coming from the Simulation Engine\\n# so it\\'s more readable in notebooks\\nfrom logging import ERROR\\nbackend_setup = {\"init_args\": {\"logging_level\": ERROR, \"log_to_driver\": False}}\\n\\n\\nclass SimpleModel(nn.Module):\\n    def __init__(self):\\n        super(SimpleModel, self).__init__()\\n        self.fc = nn.Linear(784, 128)\\n        self.relu = nn.ReLU()\\n        self.out = nn.Linear(128, 10)\\n\\n    def forward(self, x):\\n        x = torch.flatten(x, 1)\\n        x = self.fc(x)\\n        x = self.relu(x)\\n        x = self.out(x)\\n        return x\\n\\n\\ndef train_model(model, train_set):\\n    batch_size = 64\\n    num_epochs = 10\\n\\n    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\\n\\n    criterion = nn.CrossEntropyLoss()\\n    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\\n\\n    model.train()\\n    for epoch in range(num_epochs):\\n        running_loss = 0.0\\n        for inputs, labels in train_loader:\\n            optimizer.zero_grad()\\n            outputs = model(inputs)\\n            loss = criterion(outputs, labels)\\n            loss.backward()\\n            optimizer.step()\\n            running_loss += loss.item()\\n\\n\\ndef evaluate_model(model, test_set):\\n    model.eval()\\n    correct = 0\\n    total = 0\\n    total_loss = 0\\n\\n    test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\\n    criterion = nn.CrossEntropyLoss()\\n\\n    with torch.no_grad():\\n        for inputs, labels in test_loader:\\n            outputs = model(inputs)\\n            _, predicted = torch.max(outputs.data, 1)\\n            total += labels.size(0)\\n            correct += (predicted == labels).sum().item()\\n\\n            loss = criterion(outputs, labels)\\n            total_loss += loss.item()\\n\\n    accuracy = correct / total\\n    average_loss = total_loss / len(test_loader)\\n    # print(f\"Test Accuracy: {accuracy:.4f}, Average Loss: {average_loss:.4f}\")\\n    return average_loss, accuracy\\n\\n\\ndef include_digits(dataset, included_digits):\\n    including_indices = [\\n        idx for idx in range(len(dataset)) if dataset[idx][1] in included_digits\\n    ]\\n    return torch.utils.data.Subset(dataset, including_indices)\\n\\n\\ndef exclude_digits(dataset, excluded_digits):\\n    including_indices = [\\n        idx for idx in range(len(dataset)) if dataset[idx][1] not in excluded_digits\\n    ]\\n    return torch.utils.data.Subset(dataset, including_indices)\\n\\n\\ndef compute_confusion_matrix(model, testset):\\n    # Initialize lists to store true labels and predicted labels\\n    true_labels = []\\n    predicted_labels = []\\n\\n    # Iterate over the test set to get predictions\\n    for image, label in testset:\\n        # Forward pass through the model to get predictions\\n        output = model(image.unsqueeze(0))  # Add batch dimension\\n        _, predicted = torch.max(output, 1)\\n\\n        # Append true and predicted labels to lists\\n        true_labels.append(label)\\n        predicted_labels.append(predicted.item())\\n\\n    # Convert lists to numpy arrays\\n    true_labels = np.array(true_labels)\\n    predicted_labels = np.array(predicted_labels)\\n\\n    # Compute confusion matrix\\n    cm = confusion_matrix(true_labels, predicted_labels)\\n\\n    return cm\\n\\n\\ndef plot_confusion_matrix(cm, title):\\n    plt.figure(figsize=(6, 4))\\n    sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\", linewidths=0.5)\\n    plt.title(title)\\n    plt.xlabel(\"Predicted Label\")\\n    plt.ylabel(\"True Label\")\\n    plt.show()\\n'}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "#adding utils2\n",
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#installing flower\n",
        "!pip install flwr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "x7MxQnsioRCL",
        "outputId": "77ebae1c-c822-44e7-ee55-a707046972f4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flwr\n",
            "  Downloading flwr-1.14.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting cryptography<43.0.0,>=42.0.4 (from flwr)\n",
            "  Downloading cryptography-42.0.8-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting grpcio!=1.64.2,<2.0.0,<=1.64.3,>=1.60.0 (from flwr)\n",
            "  Downloading grpcio-1.64.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting iterators<0.0.3,>=0.0.2 (from flwr)\n",
            "  Downloading iterators-0.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from flwr) (1.26.4)\n",
            "Collecting pathspec<0.13.0,>=0.12.1 (from flwr)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: protobuf<5.0.0,>=4.25.2 in /usr/local/lib/python3.10/dist-packages (from flwr) (4.25.5)\n",
            "Collecting pycryptodome<4.0.0,>=3.18.0 (from flwr)\n",
            "  Downloading pycryptodome-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=6.0.2 in /usr/local/lib/python3.10/dist-packages (from flwr) (6.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from flwr) (2.32.3)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.5.0 in /usr/local/lib/python3.10/dist-packages (from flwr) (13.9.4)\n",
            "Requirement already satisfied: tomli<3.0.0,>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from flwr) (2.2.1)\n",
            "Collecting tomli-w<2.0.0,>=1.0.0 (from flwr)\n",
            "  Downloading tomli_w-1.1.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting typer<0.13.0,>=0.12.5 (from flwr)\n",
            "  Downloading typer-0.12.5-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography<43.0.0,>=42.0.4->flwr) (1.17.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->flwr) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->flwr) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->flwr) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->flwr) (2024.12.14)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.5.0->flwr) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.5.0->flwr) (2.18.0)\n",
            "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.5.0->flwr) (4.12.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<0.13.0,>=0.12.5->flwr) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<0.13.0,>=0.12.5->flwr) (1.5.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography<43.0.0,>=42.0.4->flwr) (2.22)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.5.0->flwr) (0.1.2)\n",
            "Downloading flwr-1.14.0-py3-none-any.whl (523 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m523.6/523.6 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cryptography-42.0.8-cp39-abi3-manylinux_2_28_x86_64.whl (3.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading grpcio-1.64.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading iterators-0.0.2-py3-none-any.whl (3.9 kB)\n",
            "Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Downloading pycryptodome-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomli_w-1.1.0-py3-none-any.whl (6.4 kB)\n",
            "Downloading typer-0.12.5-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tomli-w, pycryptodome, pathspec, iterators, grpcio, cryptography, typer, flwr\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.69.0\n",
            "    Uninstalling grpcio-1.69.0:\n",
            "      Successfully uninstalled grpcio-1.69.0\n",
            "  Attempting uninstall: cryptography\n",
            "    Found existing installation: cryptography 43.0.3\n",
            "    Uninstalling cryptography-43.0.3:\n",
            "      Successfully uninstalled cryptography-43.0.3\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.15.1\n",
            "    Uninstalling typer-0.15.1:\n",
            "      Successfully uninstalled typer-0.15.1\n",
            "Successfully installed cryptography-42.0.8 flwr-1.14.0 grpcio-1.64.3 iterators-0.0.2 pathspec-0.12.1 pycryptodome-3.21.0 tomli-w-1.1.0 typer-0.12.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from utils2 import *\n",
        "import flwr\n",
        "from flwr.client import Client, ClientApp, NumPyClient\n",
        "from flwr.common import ndarrays_to_parameters, Context\n",
        "from flwr.server import ServerApp, ServerConfig\n",
        "from flwr.server import ServerAppComponents\n",
        "from flwr.server.strategy import FedAvg\n",
        "from flwr.simulation import run_simulation\n"
      ],
      "metadata": {
        "id": "btxoVjz5nG52"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#recreating datasets\n",
        "trainset = datasets.MNIST(\n",
        "    \"./MNIST_data/\", download=True, train=True, transform=transform\n",
        ")\n",
        "\n",
        "total_length = len(trainset)\n",
        "split_size = total_length // 3\n",
        "torch.manual_seed(42)\n",
        "part1, part2, part3 = random_split(trainset, [split_size] * 3)\n",
        "\n",
        "part1 = exclude_digits(part1, excluded_digits=[1, 3, 7])\n",
        "part2 = exclude_digits(part2, excluded_digits=[2, 5, 8])\n",
        "part3 = exclude_digits(part3, excluded_digits=[4, 6, 9])\n",
        "\n",
        "train_sets = [part1, part2, part3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "GJWiBViuonRA",
        "outputId": "0f2c8eaa-cff5-4dd9-f2f2-7be3f3f936b3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 128MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 23.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 83.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 8.53MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST_data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#recreating testsets\n",
        "testset = datasets.MNIST(\n",
        "    \"./MNIST_data/\", download=True, train=False, transform=transform\n",
        ")\n",
        "print(\"Number of examples in `testset`:\", len(testset))\n",
        "\n",
        "testset_137 = include_digits(testset, [1, 3, 7])\n",
        "testset_258 = include_digits(testset, [2, 5, 8])\n",
        "testset_469 = include_digits(testset, [4, 6, 9])"
      ],
      "metadata": {
        "id": "t9KoM3sFo_VL",
        "outputId": "d9fe932b-6b7f-4492-874b-8051b9854ff2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of examples in `testset`: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Setter for model parameterers\n",
        "def set_weights(net, parameters):\n",
        "  params_dict = zip(net.state.dic().keys(), parameters)\n",
        "  state_dict = OrderedDict(\n",
        "      {k: torch.tensor(v) for k, v in params_dict}\n",
        "  )\n",
        "  net.load_state_dict(state_dict, strict=True)\n",
        "#Getter for model parameters\n",
        "def get_weights(net):\n",
        "  ndarrays = [\n",
        "    val.cpu().numpy() for _, val in net.state_dict().items()\n",
        "  ]\n",
        "  return ndarrays\n"
      ],
      "metadata": {
        "id": "0_Wl4oOKs6T3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set_weights:</br>\n",
        "net is a reference to the PyTorch model and parameters is a list o ndarrays. It uses the list of ndarrays to uptade all the items</br>\n",
        "Get_weights:</br>\n",
        "net is a reference to the simple PyTorch model. We iterate over the items convert each one into a numpy ndarray (vetorn-dimensional homogêneo) and return a list containing all of those ndarrays.\n"
      ],
      "metadata": {
        "id": "TyQ6NjjqF7nA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FlowerClient(NumPyClient):\n",
        "  def __init__(self, net, trainset, testset, device):\n",
        "    self.net = net\n",
        "    self.trainset = trainset\n",
        "    self.testset = testset\n",
        "\n",
        "  def fit(self, parameters, config):\n",
        "    set_weight(self.net, parameters)\n",
        "    train_model(self.net. self.trainset)\n",
        "    return get_weights(self.net), len(self.trainset), {}\n",
        "\n",
        "  def evaluate(self, parameters: NDArrays, config: Dict[str, Scalar]):\n",
        "    set_weights(self.net, parameters)\n",
        "    loss, accuracy = evaluate_model(self.net, self.testset)\n",
        "    return loss, len(self.testset), {\"accuracy\": accuracy}\n",
        ""
      ],
      "metadata": {
        "id": "iiJm5Za0uve_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "self = this"
      ],
      "metadata": {
        "id": "ne2LWXSzJLZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#client function\n",
        "def client_fn(context: Context) -> Client:\n",
        "  net = SimpleModel()\n",
        "  partition_id = int(context.node_config[\"partition-id\"])\n",
        "  client_train = train_sets[int(partition_id)]\n",
        "  client_test = testset\n",
        "  return FlowerClient(net, client_train, client_test).to_client()"
      ],
      "metadata": {
        "id": "kNKnFNdNLb_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Flower calls client_fn whenever it needs an instance of one particular client to call fit or evaluate"
      ],
      "metadata": {
        "id": "J2UDlu8KS6EP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = ClientApp(client_fn)"
      ],
      "metadata": {
        "id": "McQRh5fwRrLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ClientApp represents a client in the federated system, it is responsible to comunicate whith the central server and execute local operations"
      ],
      "metadata": {
        "id": "heQWWTGVMs2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(server_round, parameters, config):\n",
        "  net = SimpleModel()\n",
        "  set_weights(net, parameters)\n",
        "\n",
        "  _, accuracy = evaluate_model(net, testset)\n",
        "  _, accuracy137 = evaluate_model(net, testset_137)\n",
        "  _, accuracy258 = evaluate_model(net, testset_258)\n",
        "  _, accuracy469 = evaluate_model(net, testset_469)\n",
        "\n",
        "  log(INFO, \"test accuracy on all digits: %.4f\", accuracy)\n",
        "  log(INFO, \"test accuracy on [1,3,7]: %.4f\", accuracy137)\n",
        "  log(INFO, \"test accuracy on [2,5,8]: %.4f\", accuracy258)\n",
        "  log(INFO, \"test accuracy on [4,6,9]: %.4f\", accuracy469)\n",
        "\n",
        "  if server_round == 3:\n",
        "    cm = compute_confusion_matrix(net, testset)\n",
        "    plot_confusion_matriz(cm, \"Final Global Model\")\n",
        "\n",
        "  return loss,\n",
        ""
      ],
      "metadata": {
        "id": "e0Mdj_PHTF6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The evaluarte method evaluates the performance of the neural network model using the provided parameters and the test dataset"
      ],
      "metadata": {
        "id": "js7k6HnxTJDF"
      }
    }
  ]
}